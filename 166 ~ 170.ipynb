{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99086d41-849a-4087-9157-06bb255e7513",
   "metadata": {},
   "source": [
    "# 166.경사하강법\n",
    "* 신경망에는 뉴런을 흉내 낸 노드들이 입력층, 은닉층(가중값과 입력된 값을 곱한 결과로 합산), 그 다음 활성화함수(activation function)는 합산된 값을 입력신호를 출력신호로 변환한다.\n",
    "* 지금까지의 과정이 신경망 알고리즘의 feed forward 부분. 이렇게 한 번의 주기를 마치면서 역전파(Back propagation) 시작하면서 오차제곱합 또는 교차엔트로피 등의 손실함수(loss function)에 기반을 두고 오류가 감소하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f56806-7fb0-48b2-badb-ab800254e3d9",
   "metadata": {},
   "source": [
    "# 167.기울기 소실문제 (Vanishing Gradient Problem)란?\n",
    "* 다층신경망의 경우 여러 개의 은닉층을 쓰기 시작하면 신경망은 매우 복잡해진다. 기울기(Gradient) 기반의 신경망은 파라미터 값의 작은 변화가 출력값에 얼마나 많은 영향을 미칠지를 이해하는 것을 기반으로 파라미터 값을 학습시킨다.\n",
    "* 만약 파라미터 값의 변화가 출력값에 매우 작은 변화를 야기한다면 신경망은 효과 적으로 학습시킬 수 없게 되는데 이것이 바로 기울기 소실문제 (Vanishing Gradient Problem)로 발생하게 된다.\n",
    "* 기울기 소실 문제(Vanishing Gradient problem)는 역전파(Backpropagation) 알고리즘에서 처음 입력층 (input layer)으로 진행할수록 기울기가 점차적으로 작아지다가 나중에는 거의 기울기의 변화가 없어지는 문제를 말한다.\n",
    "#### 출제유형\n",
    "* 기울기 소실문제 단답형 또는 객관식 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd1a7-57d9-49e1-ac7b-43f3033d7adf",
   "metadata": {},
   "source": [
    "# 168.앙상블 모형\n",
    "* 앙상블 모형은 여러 개의 분류모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법이다.\n",
    "* 이는 적절한 표본추출법으로 데이터에서 여 러 개의 훈련용 데이터 집합을 만들어 각각의 데이터 집합에서 하나의 분류기를 만들어 앙상블하는 방법이다.\n",
    "    * ① 배깅(Bagging)\n",
    "        * Bootstrap aggregating의 준말로 원 데이터 집합으로부터 크기가 같은 표본을 여러 번 단순 임의 복원, 추출하여 각 표본(붓스트랩 표본)에 대해 분류기 (Classifiers)를 생성한 후, 그 결과를 앙상블 하는 방법이다. 반복추출 방법을 사용하기 때문에 같은 데이터가 한 표본에 여러 번 추출될 수도 있고, 어떤 데이터는 추출되지 않을 수 있다.\n",
    "     \n",
    "    * ② 부스팅(Booting)\n",
    "        * 배깅의 과정과 유사하나 붓스트랩 표본을 구성하는 sampling 과정에서 각 자료에 동일한 확률을 부여하는 것이 아니라, 분류가 잘못된 데이터에 더 큰 가중치를 주어 표본을 추출한다. Adaboosting은 가장 많이 사용되는 부스팅 알고리즘이다. 배깅과 다른 점은 분류가 잘못된 데이터에 가중치(weight)를 주어 표본을 추출한 다는 점 외에는 동일하다.\n",
    "    * ③ 랜덤 포레스트(Random forest)\n",
    "        * 배깅(Bagging)에 랜덤 과정을 추가한 방법이다. 각 노드마다 모든 예측변수 안에서 최적의 분할을 선택하는 방법 대신 예측변수들을 임의로 추출하고, 추출된 변수 내에서 최적의 분할을 만들어 나가는 방법을 사용한다.\n",
    "#### 출제유형\n",
    "* 배깅, 부스팅, 랜덤 포레스트 차이가 출제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cee34e-cd83-4cab-88ae-5d3430bc5491",
   "metadata": {},
   "source": [
    "# 169.서포트 벡터 머신(Support Vector Machine)\n",
    "* 서포트 벡터 머신은 기계학습 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도학습 모델 이며, 주로 분류와 회귀 분석을 위해 사용한다.\n",
    "* 두 카테고리 중 어느 하나에 속한 데이터의 집합이 주어졌을 때, SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어느 카테고리에 속할지 판단하는 비확률 적 이진 선형 분류 모델을 만든다.\n",
    "* 만들어진 분류 모델은 데이터가 사상된 공간에서 경계로 표현되는데 SVM 알고리즘은 그 중 가장 큰 폭을 가진 경계를 찾는 알고리즘이다.\n",
    "* SVM은 선형 분류와 더불어 비선형 분류에서도 사용될 수 있다. 비선형 분류를 하기 위해 서 주어진 데이터를 고차원 특징 공간으로 사상하는 작업이 필요한데, 이를 효율적으로 하기 위해 커널 트릭을 사용하기도 한다.\n",
    "#### 출제유형\n",
    "* SVM 기타 분석기법과 달리 모형의 과적합화가 잘 되지않고 일반화 능력이 좋다.\n",
    "* 커널트릭을 통해 비선형데이터 분류에도 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7246d226-1430-44a4-8c1c-eef4ae8b32aa",
   "metadata": {},
   "source": [
    "# 170.나이브 베이즈 분류\n",
    "*  나이브 베이즈 분류(Naive Bayes Classification) 모형은 베이즈정리에 기반한 방법으로, 사후확률(일종의 조건부 결합확률)의 계산 시 조건부 독립을 가정하여 계산을 단순화한 방법으로, 사후확률이 큰 집단으로 새로운 데이터를 분류하게 된다.\n",
    "* 조건부 독립의 가정이 비현실적인 측면이 있으나 계산이 간편하여 널리 이용되고 있다.\n",
    "* \"사후확률은 사전확률을 통해 예측할 수 있다\"라는 의미에 근거하여 분류모형을 예측한다.\n",
    "* 장점은 지도학습 환경에서 매우 효율적으로 훈련할 수 있으며, 분류에 필요한 파라미터를 추정하기 위한 training data가 매우 적어도 사용할 수 있다.\n",
    "#### 출제유형\n",
    "* 베이즈 정리 기반으로 한 머신러닝 기법을 나이브 베이즈 분류이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
